{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%!pip install git + https: //github.com/andreinechaev/nvcc4jupyter.git%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nvcc_plugin %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "    #include <stdio.h>\n",
    "    #include <stdlib.h>\n",
    "    #define N 1000\n",
    "    #define THREAD_PER_BLOCK 512\n",
    "\n",
    "    __global__ void add(int * a, int * b, int * c) {\n",
    "    int indice = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (indice < N)\n",
    "        c[indice] = a[indice] + b[indice];\n",
    "    }\n",
    "\n",
    "    int main() {\n",
    "        int * a, * b, * c;\n",
    "        int * gpu_a, * gpu_b, * gpu_c;\n",
    "        int size = N * sizeof(int);\n",
    "\n",
    "        cudaMalloc((void ** ) & gpu_a, size);\n",
    "        cudaMalloc((void ** ) & gpu_b, size);\n",
    "        cudaMalloc((void ** ) & gpu_c, size);\n",
    "\n",
    "        a = (int * ) malloc(size);\n",
    "        b = (int * ) malloc(size);\n",
    "        c = (int * ) malloc(size);\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            a[i] = i;\n",
    "        }\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            b[i] = 2 * i;\n",
    "        }\n",
    "\n",
    "        cudaMemcpy(gpu_a, a, size, cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(gpu_b, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "        add << < (N + THREAD_PER_BLOCK) / THREAD_PER_BLOCK, THREAD_PER_BLOCK >>> (gpu_a, gpu_b, gpu_c);\n",
    "\n",
    "        cudaMemcpy(c, gpu_c, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "        cudaFree(gpu_a);\n",
    "        cudaFree(gpu_b);\n",
    "        cudaFree(gpu_c);\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            printf(\"%d\\n\", c[i]);\n",
    "        }\n",
    "\n",
    "        free(a);\n",
    "        free(b);\n",
    "        free(c);\n",
    "\n",
    "        return 0;\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% cu\n",
    "// Exo2: partie 1, threads appartenant au meme bloc, donc \n",
    "// M=N=nbr_threadsTotal=capacitéVecteur et un seul bloc = 1\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define N 512\n",
    "\n",
    "__global__ void dot(int * a, int * b, int * c) {\n",
    "    __shared__ int temp[N];\n",
    "    temp[threadIdx.x] = a[threadIdx.x] * b[threadIdx.x];\n",
    "    __syncthreads();\n",
    "    // Le thread 0 effectue la somme \n",
    "    if (threadIdx.x == 0) {\n",
    "        int sum = 0;\n",
    "        for (int i = 0; i < N; i++)\n",
    "            sum += temp[i];\n",
    "        * c = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "        int * a, * b, * c;\n",
    "        int * gpu_a, * gpu_b, * gpu_c;\n",
    "        int size = N * sizeof(int);\n",
    "\n",
    "        cudaMalloc((void ** ) & gpu_a, size);\n",
    "        cudaMalloc((void ** ) & gpu_b, size);\n",
    "        cudaMalloc((void ** ) & gpu_c, sizeof(int));\n",
    "\n",
    "        a = (int * ) malloc(size);\n",
    "        b = (int * ) malloc(size);\n",
    "        c = (int * ) malloc(sizeof(int));\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            a[i] = i;\n",
    "        }\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            b[i] = 2 * i;\n",
    "        }\n",
    "\n",
    "        cudaMemcpy(gpu_a, a, size, cudaMemcpyHostToDevice);\n",
    "        cudaMemcpy(gpu_b, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "        dot << < 1, N >>> (gpu_a, gpu_b, gpu_c);\n",
    "\n",
    "        cudaMemcpy(c, gpu_c, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "        cudaFree(gpu_a);\n",
    "        cudaFree(gpu_b);\n",
    "        cudaFree(gpu_c);\n",
    "\n",
    "        printf(\"%d\\n\", * c);\n",
    "\n",
    "        free(a);\n",
    "        free(b);\n",
    "        free(c);\n",
    "\n",
    "        return 0;\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% cu\n",
    "// Exo2: partie 2, threads appartenant a déffirents blocs\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define N 2048\n",
    "#define THREAD_PER_BLOCK 512\n",
    "\n",
    "__global__ void dot(int * a, int * b, int * c) {\n",
    "    //chaque bloc possède son vecteur temp partager entre les threads du meme bloc\n",
    "    __shared__ int temp[THREAD_PER_BLOCK];\n",
    "    int indice = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    temp[threadIdx.x] = a[indice] * b[indice];\n",
    "    __syncthreads();\n",
    "\n",
    "    // Le thread 0 de chaque bloc effectue une somme locale qu'il ajoute à la somme globale \"atomiquement\" \n",
    "    if (threadIdx.x == 0) {\n",
    "        int sum = 0;\n",
    "        for (int i = 0; i < THREAD_PER_BLOCK; i++)\n",
    "            sum += temp[i];\n",
    "        atomicAdd(c, sum);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int * a, * b, * c;\n",
    "    int * gpu_a, * gpu_b, * gpu_c;\n",
    "    int size = N * sizeof(int);\n",
    "\n",
    "    cudaMalloc((void ** ) & gpu_a, size);\n",
    "    cudaMalloc((void ** ) & gpu_b, size);\n",
    "    cudaMalloc((void ** ) & gpu_c, sizeof(int));\n",
    "\n",
    "    a = (int * ) malloc(size);\n",
    "    b = (int * ) malloc(size);\n",
    "    c = (int * ) malloc(sizeof(int));\n",
    "\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        a[i] = i;\n",
    "    }\n",
    "\n",
    "    for (int i = 0; i < N; i++) {\n",
    "      b[i] = 2 * i;\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(gpu_a, a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(gpu_b, b, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dot << < N / THREAD_PER_BLOCK, THREAD_PER_BLOCK >>> (gpu_a, gpu_b, gpu_c);\n",
    "\n",
    "    cudaMemcpy(c, gpu_c, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(gpu_a);\n",
    "    cudaFree(gpu_b);\n",
    "    cudaFree(gpu_c);\n",
    "\n",
    "    printf(\"%d\\n\", * c);\n",
    "\n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(c);\n",
    "\n",
    "    return 0;\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% cu\n",
    "// Exo 3 : inverser un vecteur en utilisant differents blocs (matrice logique)\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define N 2048\n",
    "#define THREAD_PER_BLOCK 512\n",
    "\n",
    "__global__ void reverseArray(int * d_b, int * d_a) {\n",
    "    int old_id = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    int new_id = N - 1 - old_id;\n",
    "    d_b[old_id] = d_a[new_id];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "        int * h_a, * d_a, * d_b;\n",
    "        int size = N * sizeof(int);\n",
    "        h_a = (int * ) malloc(size);\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "        h_a[i] = i;\n",
    "        }\n",
    "\n",
    "        cudaMalloc((void ** ) & d_a, size);\n",
    "        cudaMalloc((void ** ) & d_b, size);\n",
    "        cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "        reverseArray << < N / THREAD_PER_BLOCK, THREAD_PER_BLOCK >>> (d_b, d_a);\n",
    "\n",
    "        cudaMemcpy(h_a, d_b, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "        cudaFree(d_a);\n",
    "        cudaFree(d_b);\n",
    "\n",
    "        for (int i = 0; i < N; i++) {\n",
    "        printf(\"%d\\n\", h_a[i]);\n",
    "        }\n",
    "\n",
    "        free(h_a);\n",
    "\n",
    "        return 0;\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu \n",
    "// parallélisation de l'algorithme du calcul nobre pi\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define NUM_BLOCKS 196\n",
    "#define NUM_THREADS 512\n",
    "\n",
    "__global__ void cal_pi(double * sum, double steps, long nb_steps) {\n",
    "    double x;\n",
    "\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (idx > 0 && idx <= nb_steps)\n",
    "        x = (idx - 0.5) * steps;\n",
    "    sum[idx] = 4.0 / (1.0 + x * x);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    long nb_steps = 100000;\n",
    "    double pi = 0.0;\n",
    "    double steps = 1.0 / (double) nb_steps;\n",
    "\n",
    "        double * sumHost, * sumDev;\n",
    "\n",
    "    dim3 dimGrid(NUM_BLOCKS, 1);\n",
    "    dim3 dimBlock(NUM_THREADS, 1);\n",
    "\n",
    "    size_t size = NUM_BLOCKS * NUM_THREADS * sizeof(double);\n",
    "\n",
    "    sumHost = (double * ) malloc(size);\n",
    "\n",
    "    cudaMalloc((void ** ) & sumDev, size);\n",
    "    cudaMemset(sumDev, 0, size);\n",
    "\n",
    "    cal_pi << < dimGrid, dimBlock >>> (sumDev, steps, nb_steps);\n",
    "\n",
    "    cudaMemcpy(sumHost, sumDev, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for (int tid = 1; tid <= nb_steps; tid++)\n",
    "        pi += sumHost[tid];\n",
    "\n",
    "    pi *= steps;\n",
    "\n",
    "    printf(\"PI=%f\\n\", pi);\n",
    "\n",
    "    free(sumHost);\n",
    "\n",
    "    cudaFree(sumDev);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% cu\n",
    "// Exo 5: multiplication scalaire de deux victeurs\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define BLOCKSIZE 16\n",
    "#define SIZE 128\n",
    "\n",
    "__global__ void vectvectshared(int * A, int * B, int * r) {\n",
    "    __shared__ int temp[SIZE];\n",
    "\n",
    "    int i = threadIdx.x;\n",
    "    int j = threadIdx.y;\n",
    "\n",
    "    int ind = j + (blockDim.x * i);\n",
    "\n",
    "    if (ind < SIZE)\n",
    "        temp[ind] = A[ind] * B[ind];\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    if (ind == 0) {\n",
    "        int sum = 0;\n",
    "        for (int i = 0; i < SIZE; i++)\n",
    "            sum += temp[i];\n",
    "        * r = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "void fill_dp_vector(int * vec, int size) {\n",
    "    int ind;\n",
    "    for (ind = 0; ind < size; ind++)\n",
    "        vec[ind] = 3 * ind;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int * hostA, * hostB, * res;\n",
    "    int * devA, * devB, * devres;\n",
    "\n",
    "    int vlen;\n",
    "\n",
    "    vlen = SIZE;\n",
    "\n",
    "    dim3 threadspblock(BLOCKSIZE, BLOCKSIZE);\n",
    "\n",
    "    hostA = (int * ) malloc(vlen * sizeof(int));\n",
    "    hostB = (int * ) malloc(vlen * sizeof(int));\n",
    "    res = (int * ) malloc(sizeof(int));\n",
    "\n",
    "    fill_dp_vector(hostA, vlen);\n",
    "    fill_dp_vector(hostB, vlen);\n",
    "\n",
    "    cudaMalloc((void ** ) & devA, vlen * sizeof(int));\n",
    "    cudaMalloc((void ** ) & devB, vlen * sizeof(int));\n",
    "    cudaMalloc((void ** ) & devres, sizeof(int));\n",
    "\n",
    "    cudaMemcpy(devA, hostA, vlen * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(devB, hostB, vlen * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    vectvectshared << < 1, threadspblock >>> (devA, devB, devres);\n",
    "\n",
    "    cudaMemcpy(res, devres, sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    cudaFree(devA);\n",
    "    cudaFree(devB);\n",
    "    cudaFree(devres);\n",
    "\n",
    "    printf(\"%d\\n\", * res);\n",
    "\n",
    "    free(hostA);\n",
    "    free(hostB);\n",
    "    free(res);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
